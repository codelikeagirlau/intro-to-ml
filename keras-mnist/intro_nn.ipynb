{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Much of this material comes from The Scientist and Engineer's Guide to Digital Signal Processing (free online version: http://www.dspguide.com/pdfbook.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "The neuron is one of the fundamental cells in the nervous system. The function of a neuron is to act as a node in a relay chain, taking inputs from connected neurons, performing processing on the input and passing it along. These connected neurons are called neural networks and have an (amongst many) interesting propery of being capable of processing poorly defined information effectively. \n",
    "\n",
    "<center>**Obligitory figure of a simplified biological neural network**</center>\n",
    "<img src=\"http://www.biologymad.com/nervoussystem/summation.jpg\">\n",
    "\n",
    "Artificial Neural Networks (herein called neural networks) are an attempt to mimic that property - albiet currently in a simple form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks consist of at least 3 hierarchical layers:\n",
    "\n",
    "**Input layer (1st layer):**\n",
    "* Passively receive data.\n",
    "\n",
    "**Hidden layer (2nd layers):**\n",
    "* Where the magic happens.\n",
    "\n",
    "**Output layer (final layer):**\n",
    "* Profit? Backpropogation. Profit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layers** can be either passive (do not modify data) or active (perform some function).\n",
    "\n",
    "Each layer consists of one or more nodes.\n",
    "\n",
    "Each node can be connected to some or all of the nodes down the hierarchy.\n",
    "\n",
    "Connections are not physical but instead consist of numerical weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/fig1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information flow can be either purely **feedforward** (as above) - also known as a Directed Acyclical Graph (DAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./images/fig4.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or feedback on itself (**Recurrent**).\n",
    "**Recurrent** networks can feedback information to previous layers or within the node itself. This gives the networks a \"memory\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./images/fig5.svg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can also be **densely** connected (all nodes connect to each other) Or **sparsely** connected (take a guess).\n",
    "\n",
    "<img src=./images/fig6.png>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs are weighted, passed to the hidden layer, summed, thresholded using an \"activation function\" (0 / 1) and passed to the output layer where a prediction from the \"pattern of activation\" in the hidden layer is made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/fig2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function in the above image can be any transformation and is called the **transfer function** / **activation function**.\n",
    "\n",
    "![](./images/fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation of a neuron can also be altered by the **bias** - you may know this as the \"intercept\". Depending on the activation function used, the bias * the function may move the function on the x axis or the y axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some math:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/single_hidden_math.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/matrix_form.png)\n",
    "\n",
    "Thanks to [Three Blue One Brown](http://www.3blue1brown.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are (like most algorithms you've been exposed to) iterative. They also tend to be supervised, but unsupervised neural networks exist.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Randomly set inital weights / bias.\n",
    "2. Run data through with weights / bias / activation function.\n",
    "3. On output check error (predicted - observed). \n",
    "4. Update weights via backpropagation (tweaking individual weights / bias and checking the effect on the overall loss function gradient).\n",
    "5. Repeat until convergence (if convergence).\n",
    "6. Profit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/neural_network-w_b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most machine learning methods we've used there's only been a couple of parameters that need to be set / updated and these govern the entire system.\n",
    "\n",
    "For neural networks each **weight** (connection) is a parameter and each instance of **bias** for the activation function is a parameter.\n",
    "\n",
    "How does it do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./images/fig7.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Decent** with a little help from **Backpropagation**\n",
    "\n",
    "Backpropagation consists of updating the weights starting from the output layer, working back through the hidden layer then to the input layer using the **partial** derivative of each neuron's error using gradient decent to guide the way.\n",
    "\n",
    "In English - We know the overall error of the model for a given iteration using a random allocation of weights. We change the value of each weight slightly and monitor the effect on the entire system. If the change in weight sends error downward, it continues in that direction until it doesn't.\n",
    "\n",
    "Eventually (hopefully) the model converges and we get an accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TERMS:\n",
    "\n",
    "* Node\n",
    "* Weight\n",
    "* Bias\n",
    "* Transfer / Activation Function\n",
    "* Input layer\n",
    "* Hidden layer\n",
    "* Output layer\n",
    "* Feedforward\n",
    "* Feedback / Recurrent\n",
    "* Dense\n",
    "* Sparse\n",
    "* Partial\n",
    "* Backpropagation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
